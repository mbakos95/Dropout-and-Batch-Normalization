# Dropout-and-Batch-Normalization
In this lesson, I explored two special types of layers in deep learning: 
The Dropout layer and the Batch Normalization layer. These layers do not contain neurons but add essential functionality to neural networks, often used in modern architectures.

Feel free to check the whole cource here: https://www.kaggle.com/learn/intro-to-deep-learning/course
